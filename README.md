# South Asia Eâ€‘Commerce Demand Forecasting on Databricks

## 1. Project Overview
This project implements an **endâ€‘toâ€‘end, productionâ€‘grade data and ML pipeline** on the Databricks Lakehouse for **SKUâ€‘level demand forecasting** using a [large South Asia eâ€‘commerce dataset](https://data.mendeley.com/datasets/ggbkd8ck3x/1) (100,000+ orders).

The solution demonstrates how raw CSV data can be transformed into **business insights and ML predictions** using Databricks best practices:
- Medallion Architecture (Bronze â†’ Silver â†’ Gold)
- Delta Lake with ACID guarantees
- Unity Catalog governance
- MLflowâ€‘tracked machine learning
- Automated Jobs orchestration

The project is designed to be **portfolioâ€‘ready** and aligned with realâ€‘world retail analytics and forecasting use cases.



## 2. Problem Statement & AI Framing

**Problem:**
Eâ€‘commerce platforms need accurate shortâ€‘term demand forecasts to optimize inventory, reduce stockâ€‘outs, and improve fulfillment efficiency.

**AI Framing:**
- **Task:** Supervised regression
- **Target:** Daily units sold per SKU
- **Granularity:** (product_id, order_date)
- **Approach:** Train a machine learning model using rolling historical demand features



## 3. Dataset

- **Source:** [South Asia Eâ€‘Commerce Dataset (CSV)](https://data.mendeley.com/datasets/ggbkd8ck3x/1)
- **Size:** ~100,000 orders
- **Key Challenges:**
  - Nonâ€‘ISO timestamps (e.g. `03.11.2015 06:44`)
  - Missing values (`NA`)
  - Malformed numeric fields (scientific notation in quantities)

These issues are intentionally handled in the pipeline to reflect **realâ€‘world data quality problems**.



## 4. Architecture (Medallion Lakehouse)

### Architecture Diagram

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Raw CSV     â”‚
        â”‚ (Eâ€‘Commerce) â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Bronze Layer â”‚
        â”‚ Delta Tables â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Silver Layer â”‚
        â”‚ Cleaned Data â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Gold Layer   â”‚
        â”‚ Features &   â”‚
        â”‚ KPIs         â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ML Training  â”‚
        â”‚ + MLflow     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸŸ¤ Bronze Layer â€“ Raw Ingestion
**Notebook:** `01_ingestion_bronze_south_asia`
- Ingests raw CSV into Delta Lake
- Minimal transformation
- Adds ingestion timestamp
- Stored as `bronze.raw_transactions`

### âšª Silver Layer â€“ Data Cleaning & Business Logic
**Notebook:** `02_transform_silver_south_asia`
- Explicit timestamp parsing with tolerant functions
- Nullâ€‘safe casting using `try_cast`
- Businessâ€‘level tables:
  - `silver.orders`
  - `silver.products`
  - `silver.daily_product_sales`

### ğŸŸ¡ Gold Layer â€“ Analytics & Features
**Notebook:** `03_gold_aggregations_south_asia`
- Rolling demand windows (7 / 14 / 30 days)
- Demand acceleration signals
- SKU performance KPIs
- MLâ€‘ready feature tables

### ğŸ”µ ML Layer â€“ Demand Forecasting
**Notebook:** `04_ml_demand_forecasting_south_asia`
- RandomForest regression model
- Timeâ€‘aware train/test split
- MLflow experiment tracking
- Unity Catalogâ€“compliant model logging



## 5. Machine Learning Details

- **Model:** RandomForestRegressor
- **Features:**
  - units_7d
  - units_14d
  - units_30d
- **Label:** units_sold
- **Metric:** RMSE

**Why Random Forest?**
- Handles nonâ€‘linear demand patterns
- Robust to noise
- Minimal feature assumptions

MLflow logs:
- Parameters
- Metrics
- Model artifact
- Input example & inferred signature



## 6. Orchestration & Automation

**Notebook:** `05_jobs_orchestration_south_asia`

- Uses `dbutils.notebook.run()`
- Single entry point for the full pipeline
- Executed via **Databricks Jobs**
- Supports scheduled or onâ€‘demand runs

Pipeline order:
1. Bronze ingestion
2. Silver transformations
3. Gold aggregations
4. ML training



## 7. Governance & Compliance

- **Unity Catalog schemas** for Bronze, Silver, Gold
- **UC Volumes** for MLflow artifacts
- No DBFS root dependency
- Suitable for shared or serverless clusters



## 8. Business Impact

This solution enables:
- SKUâ€‘level demand visibility
- Early trend detection
- Dataâ€‘driven inventory planning
- Scalable ML deployment foundation