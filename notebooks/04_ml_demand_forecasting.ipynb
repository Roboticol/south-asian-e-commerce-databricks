# Databricks notebook source
# MAGIC %sql
# MAGIC CREATE CATALOG IF NOT EXISTS main;
# MAGIC CREATE SCHEMA IF NOT EXISTS main.mlflow;
# MAGIC
# MAGIC CREATE VOLUME IF NOT EXISTS main.mlflow.tmp;
# MAGIC SHOW CATALOGS

# COMMAND ----------

# Databricks Notebook: 04_ml_demand_forecasting_south_asia
# Purpose: Train SKU-level demand forecasting model with MLflow
# Input: gold.product_demand_features

import mlflow
import mlflow.spark
import os

from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from mlflow.models import infer_signature

# =========================
# UNITY CATALOG MLflow TEMP PATH
# =========================
os.environ["MLFLOW_DFS_TMP"] = "/Volumes/main/mlflow/tmp"

GOLD_DB = "gold"

# =========================
# LOAD FEATURE TABLE
# =========================
features_df = (
    spark.table(f"{GOLD_DB}.product_demand_features")
    # Labels MUST be non-null for Spark ML
    .filter(col("units_sold").isNotNull())
    .filter(col("units_7d").isNotNull())
    .filter(col("units_14d").isNotNull())
    .filter(col("units_30d").isNotNull())
)

# ==================
# TRAIN / TEST SPLIT
# ==================
train_df = features_df.filter(col("order_date") < "2018-01-01")
test_df = features_df.filter(col("order_date") >= "2018-01-01")

from pyspark.ml.feature import VectorAssembler

feature_cols = ["units_7d", "units_14d", "units_30d"]

assembler = VectorAssembler(
    inputCols=["units_7d", "units_14d", "units_30d"],
    outputCol="features"
)

train_vec = assembler.transform(train_df)

# =========================
# FEATURE ASSEMBLY
# =========================
feature_cols = ["units_7d", "units_14d", "units_30d"]

assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)

train_vec = assembler.transform(train_df)
test_vec = assembler.transform(test_df)

# =========================
# INPUT EXAMPLE
# =========================
input_example = (
    train_df
    .select(feature_cols)
    .limit(5)
    .toPandas()
)

# =========================
# MODEL DEFINITION
# =========================
rf = RandomForestRegressor(
    featuresCol="features",
    labelCol="units_sold",
    numTrees=50,
    maxDepth=8,
    seed=42
)

# =========================
# MLflow EXPERIMENT
# =========================
mlflow.set_experiment("/Shared/south_asia_ecommerce_demand_forecasting")

with mlflow.start_run(run_name="rf_demand_forecast"):
    model = rf.fit(train_vec)

    predictions = model.transform(test_vec)

    evaluator = RegressionEvaluator(
        labelCol="units_sold",
        predictionCol="prediction",
        metricName="rmse"
    )

    rmse = evaluator.evaluate(predictions)

    signature = infer_signature(
        train_df.select(feature_cols).limit(100).toPandas(),
        predictions.select("prediction").limit(100).toPandas()
    )
    mlflow.log_param("num_trees", 50)
    mlflow.log_param("max_depth", 8)
    mlflow.log_metric("rmse", rmse)

    mlflow.spark.log_model(
        model,
        artifact_path="demand_model",
        signature=signature
    )
    
    print(f"RMSE: {rmse}")
    
# =========================
# INFERENCE SAMPLE
# =========================
predictions.select(
    "product_id",
    "order_date",
    "units_sold",
    "prediction"
).show(10)


# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT
# MAGIC   AVG(units_sold) AS mean_units,
# MAGIC   STDDEV(units_sold) AS std_units,
# MAGIC   MIN(units_sold) AS min_units,
# MAGIC   MAX(units_sold) AS max_units
# MAGIC FROM gold.product_demand_features;

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Total orders
# MAGIC SELECT COUNT(DISTINCT order_id) AS total_orders
# MAGIC FROM silver.orders;

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- Total products
# MAGIC SELECT COUNT(DISTINCT product_id) AS total_skus
# MAGIC FROM silver.products;

# COMMAND ----------



# COMMAND ----------

# MAGIC %sql
# MAGIC -- Avg daily demand
# MAGIC SELECT
# MAGIC   AVG(units_sold) AS avg_daily_units
# MAGIC FROM gold.product_demand_features;

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT
# MAGIC   MIN(order_date) AS start_date,
# MAGIC   MAX(order_date) AS end_date
# MAGIC FROM gold.product_demand_features;
# MAGIC

# COMMAND ----------

from pyspark.sql.functions import col
from pyspark.ml.evaluation import RegressionEvaluator

# Baseline prediction = rolling 7-day average (cast to double)
baseline_df = test_vec.withColumn(
    "baseline_prediction",
    col("units_7d").cast("double")
)

baseline_rmse = RegressionEvaluator(
    labelCol="units_sold",
    predictionCol="baseline_prediction",
    metricName="rmse"
).evaluate(baseline_df)

print(f"Baseline RMSE: {baseline_rmse}")
print(f"Model RMSE: {rmse}")
